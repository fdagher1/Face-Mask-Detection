{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using https://www.kaggle.com/sauravmaheshkar/neural-image-captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Could not find kaggle.json. Make sure it's located in C:\\Users\\u1\\.kaggle. Or use the environment method.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-bc3ca2d60a33>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m## Logging into Weights and Biases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkaggle\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mUserSecretsClient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0muser_secrets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUserSecretsClient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mapi_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muser_secrets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_secret\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"WANDB_API_KEY\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python39\\lib\\site-packages\\kaggle\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mapi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKaggleApi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mApiClient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauthenticate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\program files\\python39\\lib\\site-packages\\kaggle\\api\\kaggle_api_extended.py\u001b[0m in \u001b[0;36mauthenticate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    162\u001b[0m                 \u001b[0mconfig_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_config_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m                 raise IOError('Could not find {}. Make sure it\\'s located in'\n\u001b[0m\u001b[0;32m    165\u001b[0m                               ' {}. Or use the environment method.'.format(\n\u001b[0;32m    166\u001b[0m                                   self.config_file, self.config_dir))\n",
      "\u001b[1;31mOSError\u001b[0m: Could not find kaggle.json. Make sure it's located in C:\\Users\\u1\\.kaggle. Or use the environment method."
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!pip install --upgrade wandb\n",
    "\n",
    "## Importing Packages\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import transformers\n",
    "import pandas as pd \n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Callable, Optional\n",
    "\n",
    "## Logging into Weights and Biases\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "import wandb\n",
    "wandb.login(key=api_key);\n",
    "\n",
    "wandb.init(project=\"show-and-tell\", entity=\"collaborativeml\")\n",
    "\n",
    "## For Reproducibility\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(42)\n",
    "\n",
    "## Tokenizer\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\",do_lower_case=True)\n",
    "\n",
    "## Device Configuration \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic File Paths\n",
    "data_dir = 'C:/Users/u1\\Desktop/archive'\n",
    "image_dir = f'{data_dir}/flickr30k_images'\n",
    "csv_file = f'{data_dir}/results.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>comment_number</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:/Users/u1\\Desktop/archive/flickr30k_images/1...</td>\n",
       "      <td>0</td>\n",
       "      <td>Two young guys with shaggy hair look at their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:/Users/u1\\Desktop/archive/flickr30k_images/1...</td>\n",
       "      <td>1</td>\n",
       "      <td>Two young , White males are outside near many...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:/Users/u1\\Desktop/archive/flickr30k_images/1...</td>\n",
       "      <td>2</td>\n",
       "      <td>Two men in green shirts are standing in a yard .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:/Users/u1\\Desktop/archive/flickr30k_images/1...</td>\n",
       "      <td>3</td>\n",
       "      <td>A man in a blue shirt standing in a garden .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:/Users/u1\\Desktop/archive/flickr30k_images/1...</td>\n",
       "      <td>4</td>\n",
       "      <td>Two friends enjoy time spent together .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_name  comment_number  \\\n",
       "0  C:/Users/u1\\Desktop/archive/flickr30k_images/1...               0   \n",
       "1  C:/Users/u1\\Desktop/archive/flickr30k_images/1...               1   \n",
       "2  C:/Users/u1\\Desktop/archive/flickr30k_images/1...               2   \n",
       "3  C:/Users/u1\\Desktop/archive/flickr30k_images/1...               3   \n",
       "4  C:/Users/u1\\Desktop/archive/flickr30k_images/1...               4   \n",
       "\n",
       "                                             comment  \n",
       "0   Two young guys with shaggy hair look at their...  \n",
       "1   Two young , White males are outside near many...  \n",
       "2   Two men in green shirts are standing in a yard .  \n",
       "3       A man in a blue shirt standing in a garden .  \n",
       "4            Two friends enjoy time spent together .  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(csv_file, delimiter='|')\n",
    "df[' comment_number'][19999] = ' 4'\n",
    "df[' comment'][19999] = ' A dog runs across the grass .'\n",
    "df['image_name'] = image_dir+'/'+df['image_name']\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>comment_0</th>\n",
       "      <th>comment_1</th>\n",
       "      <th>comment_2</th>\n",
       "      <th>comment_3</th>\n",
       "      <th>comment_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:/Users/u1\\Desktop/archive/flickr30k_images/1...</td>\n",
       "      <td>Two young guys with shaggy hair look at their...</td>\n",
       "      <td>Two young , White males are outside near many...</td>\n",
       "      <td>Two men in green shirts are standing in a yard .</td>\n",
       "      <td>A man in a blue shirt standing in a garden .</td>\n",
       "      <td>Two friends enjoy time spent together .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:/Users/u1\\Desktop/archive/flickr30k_images/1...</td>\n",
       "      <td>Several men in hard hats are operating a gian...</td>\n",
       "      <td>Workers look down from up above on a piece of...</td>\n",
       "      <td>Two men working on a machine wearing hard hats .</td>\n",
       "      <td>Four men on top of a tall structure .</td>\n",
       "      <td>Three men on a large rig .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:/Users/u1\\Desktop/archive/flickr30k_images/1...</td>\n",
       "      <td>A child in a pink dress is climbing up a set ...</td>\n",
       "      <td>A little girl in a pink dress going into a wo...</td>\n",
       "      <td>A little girl climbing the stairs to her play...</td>\n",
       "      <td>A little girl climbing into a wooden playhouse</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:/Users/u1\\Desktop/archive/flickr30k_images/1...</td>\n",
       "      <td>Someone in a blue shirt and hat is standing o...</td>\n",
       "      <td>A man in a blue shirt is standing on a ladder...</td>\n",
       "      <td>A man on a ladder cleans the window of a tall...</td>\n",
       "      <td>man in blue shirt and jeans on ladder cleanin...</td>\n",
       "      <td>a man on a ladder cleans a window</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:/Users/u1\\Desktop/archive/flickr30k_images/1...</td>\n",
       "      <td>Two men , one in a gray shirt , one in a blac...</td>\n",
       "      <td>Two guy cooking and joking around with the ca...</td>\n",
       "      <td>Two men in a kitchen cooking food on a stove .</td>\n",
       "      <td>Two men are at the stove preparing food .</td>\n",
       "      <td>Two men are cooking a meal .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_name  \\\n",
       "0  C:/Users/u1\\Desktop/archive/flickr30k_images/1...   \n",
       "1  C:/Users/u1\\Desktop/archive/flickr30k_images/1...   \n",
       "2  C:/Users/u1\\Desktop/archive/flickr30k_images/1...   \n",
       "3  C:/Users/u1\\Desktop/archive/flickr30k_images/1...   \n",
       "4  C:/Users/u1\\Desktop/archive/flickr30k_images/1...   \n",
       "\n",
       "                                           comment_0  \\\n",
       "0   Two young guys with shaggy hair look at their...   \n",
       "1   Several men in hard hats are operating a gian...   \n",
       "2   A child in a pink dress is climbing up a set ...   \n",
       "3   Someone in a blue shirt and hat is standing o...   \n",
       "4   Two men , one in a gray shirt , one in a blac...   \n",
       "\n",
       "                                           comment_1  \\\n",
       "0   Two young , White males are outside near many...   \n",
       "1   Workers look down from up above on a piece of...   \n",
       "2   A little girl in a pink dress going into a wo...   \n",
       "3   A man in a blue shirt is standing on a ladder...   \n",
       "4   Two guy cooking and joking around with the ca...   \n",
       "\n",
       "                                           comment_2  \\\n",
       "0   Two men in green shirts are standing in a yard .   \n",
       "1   Two men working on a machine wearing hard hats .   \n",
       "2   A little girl climbing the stairs to her play...   \n",
       "3   A man on a ladder cleans the window of a tall...   \n",
       "4     Two men in a kitchen cooking food on a stove .   \n",
       "\n",
       "                                           comment_3  \\\n",
       "0       A man in a blue shirt standing in a garden .   \n",
       "1              Four men on top of a tall structure .   \n",
       "2    A little girl climbing into a wooden playhouse    \n",
       "3   man in blue shirt and jeans on ladder cleanin...   \n",
       "4          Two men are at the stove preparing food .   \n",
       "\n",
       "                                  comment_4  \n",
       "0   Two friends enjoy time spent together .  \n",
       "1                Three men on a large rig .  \n",
       "2     A girl going into a wooden building .  \n",
       "3         a man on a ladder cleans a window  \n",
       "4              Two men are cooking a meal .  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Restructuring Data\n",
    "image_name = {\n",
    "    'image_name':df[df[' comment_number'] == df[' comment_number'][0]]['image_name'].values,\n",
    "}\n",
    "comments = {\n",
    "    'comment_0':df[df[' comment_number'] == df[' comment_number'][0]][' comment'].values,\n",
    "    'comment_1':df[df[' comment_number'] == df[' comment_number'][1]][' comment'].values,\n",
    "    'comment_2':df[df[' comment_number'] == df[' comment_number'][2]][' comment'].values,\n",
    "    'comment_3':df[df[' comment_number'] == df[' comment_number'][3]][' comment'].values,\n",
    "    'comment_4':df[df[' comment_number'] == df[' comment_number'][4]][' comment'].values,\n",
    "}\n",
    "\n",
    "image_name_df = pd.DataFrame.from_dict(image_name)\n",
    "comments_df = pd.DataFrame.from_dict(comments)\n",
    "\n",
    "df = pd.concat([image_name_df,comments_df], axis=1)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19069, 6)\n",
      "(6357, 6)\n",
      "(6357, 6)\n"
     ]
    }
   ],
   "source": [
    "#Splitting into Train, Valid and Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Obtain Train and Test Split \n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "## Reset Indexes \n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "## Obtain Train and Validation Split \n",
    "train, val = train_test_split(train, test_size=0.25, random_state=42)\n",
    "\n",
    "## Reset Indexes \n",
    "train = train.reset_index(drop=True)\n",
    "val = val.reset_index(drop=True)\n",
    "\n",
    "## Let's see how many entries we have\n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class FlickrDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, transforms: Optional[Callable] = None) -> None:\n",
    "        self.df = df\n",
    "        self.transforms = T.Compose([T.ToTensor(), T.Normalize(mean = [0.5], std = [0.5]), T.Resize((256,256)),])\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        \n",
    "        image_id = self.df.image_name.values[idx]\n",
    "        image = Image.open(image_id).convert('RGB')\n",
    "            \n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "            \n",
    "        comments = self.df[self.df.image_name == image_id].values.tolist()[0][1:][0] # Last zero is to obtain the first caption ONLY\n",
    "        encoded_inputs = tokenizer(comments, return_token_type_ids = False, return_attention_mask = False, max_length = 100, padding = \"max_length\", return_tensors = \"pt\")\n",
    "        \n",
    "        sample = {\"image\":image.to(device),\"captions\": encoded_inputs[\"input_ids\"].flatten().to(device)}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = FlickrDataset(train, transforms = True)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, drop_last=True)\n",
    "\n",
    "val_dataset = FlickrDataset(val, transforms = True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size = batch_size,drop_last=True)\n",
    "\n",
    "test_dataset = FlickrDataset(test, transforms = True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_size):\n",
    "        super(CNN, self).__init__()\n",
    "        model = models.resnet50(pretrained=True)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        \n",
    "        modules = list(model.children())[:-1]\n",
    "        self.model = nn.Sequential(*modules)\n",
    "        self.embed = nn.Linear(model.fc.in_features, embed_size)\n",
    "        \n",
    "    def forward(self, image):\n",
    "        features = self.model(image)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.embed(features)\n",
    "                \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, embedding_dim,vocab_size):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings = vocab_size,embedding_dim = embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def init_hidden(self, features):\n",
    "        \n",
    "        return (torch.autograd.Variable(torch.zeros(1,32,512).to(device)), \n",
    "                torch.autograd.Variable(features.unsqueeze(0)).to(device))\n",
    "        \n",
    "    def forward(self, features, captions):\n",
    "        \n",
    "        state = self.init_hidden(features)\n",
    "        \n",
    "        embed = self.embedding(captions)\n",
    "                    \n",
    "        lstm_out, state = self.lstm(embed, state)\n",
    "                        \n",
    "        outputs = self.fc(lstm_out)\n",
    "        outputs = outputs.view(-1, self.vocab_size)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-0e353a049106>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mexample_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexample_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"image\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexample_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"captions\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m512\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-3d26b0a28127>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mcomments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_name\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mimage_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# Last zero is to obtain the first caption ONLY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mencoded_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_token_type_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_attention_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"max_length\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"image\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"captions\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mencoded_inputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_ids\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "example_batch = next(iter(train_dataloader))\n",
    "\n",
    "image, captions = example_batch[\"image\"], example_batch[\"captions\"]\n",
    "\n",
    "encoder = CNN(embed_size = 512).to(device)\n",
    "decoder = RNN(input_size = 512, hidden_size = 512, embedding_dim=512, vocab_size = 28881).to(device)\n",
    "\n",
    "features = encoder(image)\n",
    "embed = decoder(features, captions)\n",
    "\n",
    "print(\"Image Transformation: \", image.shape, \" --> \", features.shape)\n",
    "print(\"Captions Transformation: \", captions.shape, \" --> \", embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
