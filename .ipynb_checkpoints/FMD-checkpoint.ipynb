{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e0db32b",
   "metadata": {},
   "source": [
    "#from: https://towardsdatascience.com/covid-19-face-mask-detection-using-tensorflow-and-opencv-702dd833515b\n",
    "\n",
    "#which pointed to this: https://github.com/mk-gurucharan/Face-Mask-Detection/blob/master/FaceMask-Detection.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9070311d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from shutil import copyfile\n",
    "from os import getcwd\n",
    "from os import listdir\n",
    "import cv2\n",
    "from tensorflow.keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "import imutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image  as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e8d2180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of images with facemask labelled 'yes': 690\n",
      "The number of images with facemask labelled 'no': 686\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of images with facemask labelled 'yes':\",len(os.listdir('facemask-dataset/yes')))\n",
    "print(\"The number of images with facemask labelled 'no':\",len(os.listdir('facemask-dataset/no')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb77e321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 1376\n",
      "Percentage of positive examples: 50.145348837209305%, number of pos examples: 690\n",
      "Percentage of negative examples: 49.854651162790695%, number of neg examples: 686\n"
     ]
    }
   ],
   "source": [
    "def data_summary(main_path):\n",
    "    \n",
    "    yes_path = main_path+'yesreal'\n",
    "    no_path = main_path+'noreal'\n",
    "        \n",
    "    # number of files (images) that are in the the folder named 'yes' that represent tumorous (positive) examples\n",
    "    m_pos = len(listdir(yes_path))\n",
    "    # number of files (images) that are in the the folder named 'no' that represent non-tumorous (negative) examples\n",
    "    m_neg = len(listdir(no_path))\n",
    "    # number of all examples\n",
    "    m = (m_pos+m_neg)\n",
    "    \n",
    "    pos_prec = (m_pos* 100.0)/ m\n",
    "    neg_prec = (m_neg* 100.0)/ m\n",
    "    \n",
    "    print(f\"Number of examples: {m}\")\n",
    "    print(f\"Percentage of positive examples: {pos_prec}%, number of pos examples: {m_pos}\") \n",
    "    print(f\"Percentage of negative examples: {neg_prec}%, number of neg examples: {m_neg}\") \n",
    "    \n",
    "augmented_data_path = 'facemask-dataset/trial1/augmented data1/'    \n",
    "data_summary(augmented_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb287448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
    "    dataset = []\n",
    "    \n",
    "    for unitData in os.listdir(SOURCE):\n",
    "        data = SOURCE + unitData\n",
    "        if(os.path.getsize(data) > 0):\n",
    "            dataset.append(unitData)\n",
    "        else:\n",
    "            print('Skipped ' + unitData)\n",
    "            print('Invalid file i.e zero size')\n",
    "    \n",
    "    train_set_length = int(len(dataset) * SPLIT_SIZE)\n",
    "    test_set_length = int(len(dataset) - train_set_length)\n",
    "    shuffled_set = random.sample(dataset, len(dataset))\n",
    "    train_set = dataset[0:train_set_length]\n",
    "    test_set = dataset[-test_set_length:]\n",
    "       \n",
    "    for unitData in train_set:\n",
    "        temp_train_set = SOURCE + unitData\n",
    "        final_train_set = TRAINING + unitData\n",
    "        copyfile(temp_train_set, final_train_set)\n",
    "    \n",
    "    for unitData in test_set:\n",
    "        temp_test_set = SOURCE + unitData\n",
    "        final_test_set = TESTING + unitData\n",
    "        copyfile(temp_test_set, final_test_set)\n",
    "        \n",
    "        \n",
    "YES_SOURCE_DIR = \"facemask-dataset/trial1/augmented data1/yesreal/\"\n",
    "TRAINING_YES_DIR = \"facemask-dataset/trial1/augmented data1/training/yes1/\"\n",
    "TESTING_YES_DIR = \"facemask-dataset/trial1/augmented data1/testing/yes1/\"\n",
    "NO_SOURCE_DIR = \"facemask-dataset/trial1/augmented data1/noreal/\"\n",
    "TRAINING_NO_DIR = \"facemask-dataset/trial1/augmented data1/training/no1/\"\n",
    "TESTING_NO_DIR = \"facemask-dataset/trial1/augmented data1/testing/no1/\"\n",
    "split_size = .8\n",
    "split_data(YES_SOURCE_DIR, TRAINING_YES_DIR, TESTING_YES_DIR, split_size)\n",
    "split_data(NO_SOURCE_DIR, TRAINING_NO_DIR, TESTING_NO_DIR, split_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6298954b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of images with facemask in the training set labelled 'yes': 552\n",
      "The number of images with facemask in the test set labelled 'yes': 138\n",
      "The number of images without facemask in the training set labelled 'no': 548\n",
      "The number of images without facemask in the test set labelled 'no': 138\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"The number of images with facemask in the training set labelled 'yes':\", len(os.listdir('facemask-dataset/trial1/augmented data1/training/yes1')))\n",
    "print(\"The number of images with facemask in the test set labelled 'yes':\", len(os.listdir('facemask-dataset/trial1/augmented data1/testing/yes1')))\n",
    "print(\"The number of images without facemask in the training set labelled 'no':\", len(os.listdir('facemask-dataset/trial1/augmented data1/training/no1')))\n",
    "print(\"The number of images without facemask in the test set labelled 'no':\", len(os.listdir('facemask-dataset/trial1/augmented data1/testing/no1')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afc4d4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(100, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(100, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99319b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1100 images belonging to 2 classes.\n",
      "Found 276 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DIR = \"facemask-dataset/trial1/augmented data1/training\"\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(TRAINING_DIR, \n",
    "                                                    batch_size=10, \n",
    "                                                    target_size=(150, 150))\n",
    "VALIDATION_DIR = \"facemask-dataset/trial1/augmented data1/testing\"\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR, \n",
    "                                                         batch_size=10, \n",
    "                                                         target_size=(150, 150))\n",
    "checkpoint = ModelCheckpoint('model-{epoch:03d}.model',monitor='val_loss',verbose=0,save_best_only=True,mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ca5f1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "110/110 [==============================] - 48s 434ms/step - loss: 0.6359 - acc: 0.6473 - val_loss: 0.2843 - val_acc: 0.9167\n",
      "INFO:tensorflow:Assets written to: model-001.model\\assets\n",
      "Epoch 2/25\n",
      "110/110 [==============================] - 46s 419ms/step - loss: 0.4267 - acc: 0.8391 - val_loss: 0.1821 - val_acc: 0.9384\n",
      "INFO:tensorflow:Assets written to: model-002.model\\assets\n",
      "Epoch 3/25\n",
      "110/110 [==============================] - 48s 432ms/step - loss: 0.2742 - acc: 0.8918 - val_loss: 0.1695 - val_acc: 0.9565\n",
      "INFO:tensorflow:Assets written to: model-003.model\\assets\n",
      "Epoch 4/25\n",
      "110/110 [==============================] - 47s 430ms/step - loss: 0.2270 - acc: 0.9116 - val_loss: 0.1299 - val_acc: 0.9601\n",
      "INFO:tensorflow:Assets written to: model-004.model\\assets\n",
      "Epoch 5/25\n",
      "110/110 [==============================] - 47s 430ms/step - loss: 0.2100 - acc: 0.9108 - val_loss: 0.1083 - val_acc: 0.9674\n",
      "INFO:tensorflow:Assets written to: model-005.model\\assets\n",
      "Epoch 6/25\n",
      "110/110 [==============================] - 47s 426ms/step - loss: 0.2473 - acc: 0.9096 - val_loss: 0.1433 - val_acc: 0.9601\n",
      "Epoch 7/25\n",
      "110/110 [==============================] - 47s 429ms/step - loss: 0.1754 - acc: 0.9241 - val_loss: 0.1621 - val_acc: 0.9674\n",
      "Epoch 8/25\n",
      "110/110 [==============================] - 47s 425ms/step - loss: 0.2344 - acc: 0.9207 - val_loss: 0.1971 - val_acc: 0.9601\n",
      "Epoch 9/25\n",
      "110/110 [==============================] - 47s 424ms/step - loss: 0.2846 - acc: 0.8879 - val_loss: 0.1321 - val_acc: 0.9601\n",
      "Epoch 10/25\n",
      "110/110 [==============================] - 47s 425ms/step - loss: 0.1877 - acc: 0.9279 - val_loss: 0.1463 - val_acc: 0.9565\n",
      "Epoch 11/25\n",
      "110/110 [==============================] - 47s 425ms/step - loss: 0.1949 - acc: 0.9380 - val_loss: 0.1292 - val_acc: 0.9601\n",
      "Epoch 12/25\n",
      "110/110 [==============================] - 47s 423ms/step - loss: 0.1706 - acc: 0.9476 - val_loss: 0.1415 - val_acc: 0.9493\n",
      "Epoch 13/25\n",
      "110/110 [==============================] - 47s 427ms/step - loss: 0.1891 - acc: 0.9281 - val_loss: 0.0937 - val_acc: 0.9565\n",
      "INFO:tensorflow:Assets written to: model-013.model\\assets\n",
      "Epoch 14/25\n",
      "110/110 [==============================] - 46s 422ms/step - loss: 0.1194 - acc: 0.9629 - val_loss: 0.1072 - val_acc: 0.9674\n",
      "Epoch 15/25\n",
      "110/110 [==============================] - 47s 425ms/step - loss: 0.1225 - acc: 0.9572 - val_loss: 0.1177 - val_acc: 0.9710\n",
      "Epoch 16/25\n",
      "110/110 [==============================] - 46s 422ms/step - loss: 0.1106 - acc: 0.9620 - val_loss: 0.1077 - val_acc: 0.9783\n",
      "Epoch 17/25\n",
      "110/110 [==============================] - 47s 429ms/step - loss: 0.1049 - acc: 0.9697 - val_loss: 0.0612 - val_acc: 0.9783\n",
      "INFO:tensorflow:Assets written to: model-017.model\\assets\n",
      "Epoch 18/25\n",
      "110/110 [==============================] - 47s 424ms/step - loss: 0.0927 - acc: 0.9653 - val_loss: 0.0905 - val_acc: 0.9638\n",
      "Epoch 19/25\n",
      "110/110 [==============================] - 47s 423ms/step - loss: 0.0886 - acc: 0.9714 - val_loss: 0.1086 - val_acc: 0.9457\n",
      "Epoch 20/25\n",
      "110/110 [==============================] - 46s 421ms/step - loss: 0.0727 - acc: 0.9778 - val_loss: 0.0841 - val_acc: 0.9710\n",
      "Epoch 21/25\n",
      "110/110 [==============================] - 47s 423ms/step - loss: 0.1021 - acc: 0.9676 - val_loss: 0.0514 - val_acc: 0.9855\n",
      "INFO:tensorflow:Assets written to: model-021.model\\assets\n",
      "Epoch 22/25\n",
      "110/110 [==============================] - 46s 422ms/step - loss: 0.1366 - acc: 0.9572 - val_loss: 0.0610 - val_acc: 0.9819\n",
      "Epoch 23/25\n",
      "110/110 [==============================] - 47s 425ms/step - loss: 0.0902 - acc: 0.9644 - val_loss: 0.0556 - val_acc: 0.9891\n",
      "Epoch 24/25\n",
      "110/110 [==============================] - 47s 431ms/step - loss: 0.0913 - acc: 0.9765 - val_loss: 0.1102 - val_acc: 0.9710\n",
      "Epoch 25/25\n",
      "110/110 [==============================] - 46s 421ms/step - loss: 0.0818 - acc: 0.9804 - val_loss: 0.5176 - val_acc: 0.8877\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                              epochs=25,\n",
    "                              validation_data=validation_generator,\n",
    "                              callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d513e019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: savedmodel\\assets\n"
     ]
    }
   ],
   "source": [
    "#model.save('savedmodel')\n",
    "#model = keras.models.load_model('savedmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "deee06e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_clsfr=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e03bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict={0:'with_mask',1:'without_mask'}\n",
    "color_dict={0:(0,255,0),1:(0,0,255)}\n",
    "\n",
    "size = 4\n",
    "webcam = cv2.VideoCapture(0) #Use camera 0\n",
    "\n",
    "# We load the xml file\n",
    "classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "temp = 0\n",
    "\n",
    "while True:\n",
    "    (rval, im) = webcam.read()\n",
    "    im=cv2.flip(im,1,1) #Flip to act as a mirror\n",
    "\n",
    "    # Resize the image to speed up detection\n",
    "    mini = cv2.resize(im, (im.shape[1] // size, im.shape[0] // size))\n",
    "\n",
    "    # detect MultiScale / faces \n",
    "    faces = classifier.detectMultiScale(mini)\n",
    "\n",
    "    # Draw rectangles around each face\n",
    "    for f in faces:\n",
    "        (x, y, w, h) = [v * size for v in f] #Scale the shapesize backup\n",
    "        #Save just the rectangle faces in SubRecFaces\n",
    "        face_img = im[y:y+h, x:x+w]\n",
    "        temp = face_img\n",
    "        resized=cv2.resize(face_img,(150,150))\n",
    "        normalized=resized/255.0\n",
    "        reshaped=np.reshape(normalized,(1,150,150,3))\n",
    "        reshaped = np.vstack([reshaped])\n",
    "        result=model.predict(reshaped)\n",
    "        #print(result)\n",
    "        \n",
    "        label=np.argmax(result,axis=1)[0]\n",
    "      \n",
    "        cv2.rectangle(im,(x,y),(x+w,y+h),color_dict[label],2)\n",
    "        cv2.rectangle(im,(x,y-40),(x+w,y),color_dict[label],-1)\n",
    "        cv2.putText(im, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "        \n",
    "    # Show the image\n",
    "    cv2.imshow('LIVE',   im)\n",
    "    key = cv2.waitKey(10)\n",
    "    # if Esc key is press then break out of the loop \n",
    "    if key == 27: #The Esc key\n",
    "        break\n",
    "# Stop video\n",
    "webcam.release()\n",
    "\n",
    "# Close all started windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "775aa524",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected Ptr<cv::UMat> for argument 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-d16d9c2d65dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtest_img_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'facemask-dataset/yes/0-with-mask.jpg'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_img_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mnew_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m150\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mnormalized\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_image\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected Ptr<cv::UMat> for argument 'src'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "test_img_path = 'facemask-dataset/yes/0-with-mask.jpg'\n",
    "image = Image.open(test_img_path)\n",
    "new_image = cv2.resize(image, (150, 150))\n",
    "\n",
    "normalized=new_image/255\n",
    "reshaped=np.reshape(normalized,(1,150,150,3))\n",
    "reshaped = np.vstack([reshaped])\n",
    "\n",
    "plt.imshow(reshaped)\n",
    "mat = np.array(reshaped)\n",
    "result=model.predict(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "639234a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "shape",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-1c83375060d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\program files\\python39\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    539\u001b[0m             )\n\u001b[0;32m    540\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_category\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: shape"
     ]
    }
   ],
   "source": [
    "image.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
